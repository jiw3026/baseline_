{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiw3026/baseline_/blob/main/04_1_%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%EC%A0%95%EB%A6%AC(%EC%98%81%EC%96%B4%EB%B0%8F%ED%95%9C%EA%B5%AD%EC%96%B4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi20RKmSio8x"
      },
      "source": [
        "#텍스트 전처리(Text Preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXjGCQhziz_d"
      },
      "source": [
        "- 텍스트를 자연어 처리를 위해 용도에 맞도록 사전에 표준화 하는 작업\n",
        "- 텍스트 내 정보를 유지하고, 중복을 제거하여 분석 효율성을 높이기 위해 전처리를 수행\n",
        "\n",
        "1) 토큰화(Tokenizing)\n",
        "- 텍스트를 자연어 처리를 위해 분리하는 것\n",
        "- 토큰화는 단어별로 분리하는 \"단어 토큰화(Word Tokenization)\"과 문장별로 분리하는 \"문장 토큰화(Sentence Tokenization)\"으로 구분  \n",
        "=> 이후 실습에서는 단어 토큰화를 \"토큰화\"로 통일하여 칭함\n",
        "\n",
        "2) 품사 부착(Pos Tagging)\n",
        "- 각 토큰에 품사 정보를 추가\n",
        "- 분석시에 불필요한 품사를 제거하거나(예. 조사, 접속사 등) 필요한 품사를 필터링 하기 위해 사용\n",
        "\n",
        "3) 개체명 인식(NER, Named Entity Recognition)\n",
        "- 각 토큰의 개체구분(기관, 인물, 지역, 날짜 등) 태그를 부착\n",
        "- 텍스트가 무엇과 관련되어 있는지 구분하기 위해 사용\n",
        "- 예를 들어, 과일의 apple과 기업의 apple을 구분하는 방법이 개체명 인식임. \n",
        "\n",
        "4) 원형복원(Stemming & Lemmatization)\n",
        "- 각 토큰의 원형을 복원함으로써 토큰을 표준화 하여 불필요한 데이터 중복을 방지  \n",
        "(= 단어의 수를 줄일 수 있어 연산 효율성을 높임)  \n",
        " \n",
        " - 어간추출(Stemming): 품사를 무시하고 규칙에 기반하여 어간을 추출\n",
        " - 표제어 추출(Lemmatization): 품사정보를 유지하여 표제어 추출\n",
        "\n",
        "5) 불용어 처리(Stopword)\n",
        " - 자연어 처리를 위해 불필요한 요소를 제거하는 작업\n",
        " - 불필요한 품사를 제거하는 작업과 불필요한 단어를 제거하는 작업으로 구성\n",
        " - 불필요한 토큰을 제거함으로써 연산의 효율성을 높임"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7WKR67tj-j9"
      },
      "source": [
        "#1. 영문 전처리 연습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSfmG8RxkD_Y"
      },
      "source": [
        "NLTK lib(https://www.nltk.org/)사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E32YHTz0kJIr"
      },
      "source": [
        "## 1.1 실습용 영문기사 수집\n",
        "\n",
        "온라인 기사를 바로 수집하여 실습데이터로 사용\n",
        "https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/#4a0130481f25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-hq5fi6keZZ"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3cyyQxSkieU"
      },
      "source": [
        "url = 'https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/#4a0130481f25'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCd6NTG8kpco",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "c723d643-41d2-4492-d89d-feb4d6c63eaa"
      },
      "source": [
        "eng_news = soup.select('p') #[class = 'speakable-paragraph']\n",
        "eng_text = eng_news[3].get_text()\n",
        "\n",
        "eng_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"And yes, she does mean everybody's job from yours to mine and onward to the role of grain farmers in Egypt, pastry chefs in Paris and dog walkers in Oregon i.e. every job. We will now be able to help direct all workers’ actions and behavior with a new degree of intelligence that comes from predictive analytics, all stemming from the AI engines we will now increasingly depend upon.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs6j_xjrk1Zn"
      },
      "source": [
        "## 1.2 영문토큰화 \n",
        "https://www.nltk.org/api/nltk.tokenize.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0poQI4Fk4bF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b49685-2663-4e57-f664-a0a73ea4b23f"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhzpaYvYk8w5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c11b6c69-5927-4c0b-d171-fde330bb178a"
      },
      "source": [
        "#word_tokenize(): 단어와 구두점(온점, 컴마, 물음표, 세미콜론, 느낌표 등과 같은 기호)으로\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "text =  \"James is working at Disney in London\"\n",
        "word_tokens = word_tokenize(text)\n",
        "print(word_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['James', 'is', 'working', 'at', 'Disney', 'in', 'London']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ateuSI0clYAP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "139a4982-1260-4e21-fe37-c434813f4c65"
      },
      "source": [
        "#word_tokenize(): 단어와 구두점(온점, 컴마, 물음표, 세미콜론, 느낌표 등과 같은 기호)으로\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "token1 = word_tokenize(eng_text)\n",
        "print(token1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'s\", 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i.e', '.', 'every', 'job', '.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r4iZyTBlqeT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b12c6b7-4458-4083-ad9e-f92559e562ca"
      },
      "source": [
        "#WordPunctTokenizer(): 알파벳과 알파벳이 아닌 문자를 구분하여 토큰화\n",
        "import nltk\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "text =  \"James is working at Disney in London\"\n",
        "wordpuncttoken = WordPunctTokenizer().tokenize(text)\n",
        "print(wordpuncttoken)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['James', 'is', 'working', 'at', 'Disney', 'in', 'London']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKBHE9TjlyES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa3f537c-b141-485a-c3f7-3bbe34b997ae"
      },
      "source": [
        "#TreebankWordTokenizer(): 정규표현식에 기반한 토큰화\n",
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "text =  \"James is working at Disney in London\"\n",
        "treebankwordtoken = TreebankWordTokenizer().tokenize(text)\n",
        "print(treebankwordtoken)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['James', 'is', 'working', 'at', 'Disney', 'in', 'London']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo-tGurmmPOI"
      },
      "source": [
        "## 1.3 영문 품사 부착(PoS tagging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOxdStzHmTPD"
      },
      "source": [
        "분리한 토큰마다 품사를 부착한다.   \n",
        "https://www.nltk.org/api/nltk.tag.html\n",
        "\n",
        "태그목록: https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY4p27-6mdXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a1039a2-9765-44a7-fcf1-9eda307200e7"
      },
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "taggedToken = pos_tag(word_tokens)\n",
        "print(taggedToken)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('James', 'NNP'), ('is', 'VBZ'), ('working', 'VBG'), ('at', 'IN'), ('Disney', 'NNP'), ('in', 'IN'), ('London', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIyviWpQmszG"
      },
      "source": [
        "##1.4 개체명 인식(NER, Named Entity Recognition)\n",
        "http://www.nltk.org/api/nltk.chunk.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "개체명 인식(Named Entity Recognition)이란 말 그대로 이름을 가진 개체(named entity)를 인식하겠다는 것을 의미합니다.   \n",
        "즉, 어떤 이름을 의미하는 단어를 보고는 그 단어가 어떤 유형인지를 인식하는 것을 말합니다."
      ],
      "metadata": {
        "id": "KwU2RBaGYWPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "예를 들어 \b제임스는 2018년에 골드만삭스에 입사했다. 라는 문장이 있을 때, 사람(person), 조직(organization), 시간(time)에 대해 개체명 인식을 수행하는 모델이라면 다음과 같은 결과를 보여줍니다.\n",
        "\n",
        "- 제임스 : 사람\n",
        "- 2018년 : 시간\n",
        "- 골드만삭스 : 조직"
      ],
      "metadata": {
        "id": "oUxiL3o_YdYw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKLlSBjPmzey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9bb8857-f3b2-4e43-9480-224df4887c38"
      },
      "source": [
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDAZMQzCm4AZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c154c773-a4f7-43e5-85a7-397e9c9aa089"
      },
      "source": [
        "from nltk import ne_chunk\n",
        "neToken = ne_chunk(taggedToken)\n",
        "print(neToken)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON James/NNP)\n",
            "  is/VBZ\n",
            "  working/VBG\n",
            "  at/IN\n",
            "  (ORGANIZATION Disney/NNP)\n",
            "  in/IN\n",
            "  (GPE London/NNP))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "James는 PERSON(사람), Disney는 조직(ORGANIZATION), London은 위치(GPE)"
      ],
      "metadata": {
        "id": "wesTWfS5ZBUW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvrF1piqm93I"
      },
      "source": [
        "## 1.5 원형복원\n",
        "각 토큰의 원형을 복원하여 표준화한다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIn9P5aWnExP"
      },
      "source": [
        "### 1.5.1 어간추출(Stemming)\n",
        "- 규칙에 기반하여 토큰을 표준화\n",
        "- ing제거, ful제거 등  \n",
        "http://www.nltk.org/api/nltk.chunk.html\n",
        "\n",
        "규칙 상세: http://www.nltk.org/api/nltk.chunk.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzmVDuNjnQC2"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4umNUPgnWCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78dc6533-399e-490a-9b43-de9ab99c2d81"
      },
      "source": [
        "print(\"running -> \" + ps.stem(\"running\"))\n",
        "print(\"believes -> \"+ps.stem('believes'))\n",
        "print('using ->' + ps.stem(\"using\"))\n",
        "print(\"conversation ->\" + ps.stem('conversation'))\n",
        "print('organization ->'+ ps.stem('organization'))\n",
        "print('studies -> '+ ps.stem(\"studies\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running -> run\n",
            "believes -> believ\n",
            "using ->use\n",
            "conversation ->convers\n",
            "organization ->organ\n",
            "studies -> studi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qsEKIQFxpLfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4q19tr7nssB"
      },
      "source": [
        "### 1.5.2 표제어 추출(Lemmatization)\n",
        "- 품사정보를 보존하여 토큰을 표준화\n",
        "\n",
        "http://www.nltk.org/api/nltk.chunk.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmHHrOGYnz75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f403ec75-4485-4bd2-afa0-f808455e6cbd"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W__Dwo5vn3LT"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wl = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP54tNuCn8Xr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b1c1b52-2188-4c2a-a980-0c57213a1841"
      },
      "source": [
        "print(\"running -> \" + wl.lemmatize(\"running\"))\n",
        "print(\"believes -> \"+wl.lemmatize('believes'))\n",
        "print('using ->' + wl.lemmatize(\"using\"))\n",
        "print(\"conversation ->\" + wl.lemmatize('conversation'))\n",
        "print('organization ->'+ wl.lemmatize('organization'))\n",
        "print('studies -> '+ wl.lemmatize(\"studies\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running -> running\n",
            "believes -> belief\n",
            "using ->using\n",
            "conversation ->conversation\n",
            "organization ->organization\n",
            "studies -> study\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjM5QL_joCBW"
      },
      "source": [
        "## 1.6 불용어 처리(Stopword)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u65AMKkDoKZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a49a72a8-1a62-4192-e4bc-d34f44828354"
      },
      "source": [
        "stopPos = [\"IN\", \"CC\", \"UH\", \"TO\", \"MD\", \"DT\", \"VBZ\", \"VBP\"]\n",
        "\n",
        "# 최빈어 조회: 최빈어를 조회하여 불용어 제거 대상을 선정\n",
        "from collections import Counter\n",
        "Counter(taggedToken).most_common()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('James', 'NNP'), 1),\n",
              " (('is', 'VBZ'), 1),\n",
              " (('working', 'VBG'), 1),\n",
              " (('at', 'IN'), 1),\n",
              " (('Disney', 'NNP'), 1),\n",
              " (('in', 'IN'), 1),\n",
              " (('London', 'NNP'), 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SXnMij3oaZN"
      },
      "source": [
        "stopWord = [\",\", \"be\", \"able\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55cajMcYoeIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84cc43b7-c73a-4af9-a74d-d42235a2c8d4"
      },
      "source": [
        "word = []\n",
        "for tag in taggedToken:\n",
        "  if tag[1] not in stopPos:\n",
        "    if tag[0] not in stopWord:\n",
        "      word.append(tag[0])\n",
        "\n",
        "print(word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['James', 'working', 'Disney', 'London']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn7L86ghon40"
      },
      "source": [
        "## 1.7 영문 텍스트 전처리 종합"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxHDr8Aeow1O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f6b2ec6-4391-4894-dc89-d65c63aa2556"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger') #pos tagging\n",
        "nltk.download('words') #NER\n",
        "nltk.download('maxnet_ne_chuncker') #NER\n",
        "nltk.download('wordnet') #Lemmatization\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "sumtoken = TreebankWordTokenizer().tokenize(\"Obama loves fried chicken of KFC\")\n",
        "print(sumtoken)\n",
        "\n",
        "from nltk import pos_tag\n",
        "sumTaggedToken = pos_tag(sumtoken)\n",
        "print(sumTaggedToken)\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "print(\"loves -> \" + ps.stem(\"loves\"))\n",
        "print(\"fried => \" + ps.stem('fried'))\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        "print(\"loves -> \" + wl.lemmatize(\"loves\"))\n",
        "print(\"fried => \" + wl.lemmatize('fried'))\n",
        "\n",
        "#불용어 처리\n",
        "sumStopPos = ['IN']\n",
        "sumStopWord = [\"fried\"]\n",
        "\n",
        "word = []\n",
        "for tag in taggedToken:\n",
        "  if tag[1] not in stopPos:\n",
        "    if tag[0] not in stopWord:\n",
        "      word.append(tag[0])\n",
        "\n",
        "print(word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Obama', 'loves', 'fried', 'chicken', 'of', 'KFC']\n",
            "[('Obama', 'NNP'), ('loves', 'VBZ'), ('fried', 'VBN'), ('chicken', 'NN'), ('of', 'IN'), ('KFC', 'NNP')]\n",
            "loves -> love\n",
            "fried => fri\n",
            "loves -> love\n",
            "fried => fried\n",
            "['James', 'working', 'Disney', 'London']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Error loading maxnet_ne_chuncker: Package\n",
            "[nltk_data]     'maxnet_ne_chuncker' not found in index\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.8 Tensorflow - Text preprocessing"
      ],
      "metadata": {
        "id": "qndSYJgGw6MV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.8.1 Tokenizer \n",
        "- https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"
      ],
      "metadata": {
        "id": "9jNICO8AxNtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eng_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "M9hMbti7xH89",
        "outputId": "dda0c6a1-b654-4915-8ee6-50cfa9c39393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"And yes, she does mean everybody's job from yours to mine and onward to the role of grain farmers in Egypt, pastry chefs in Paris and dog walkers in Oregon i.e. every job. We will now be able to help direct all workers’ actions and behavior with a new degree of intelligence that comes from predictive analytics, all stemming from the AI engines we will now increasingly depend upon.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "\n",
        "tokenizer = Tokenizer(split=' ', char_level=False) # 토큰화 함수 지정\n",
        "\n",
        "# 단어 인덱스 구축\n",
        "tokenizer.fit_on_texts([eng_text])"
      ],
      "metadata": {
        "id": "WpLUoRi7w6GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vocab = tokenizer.word_index # 딕셔너리 형태\n",
        "print(\"전체 단어 개수: \", len(word_vocab)) # 전체 단어 개수 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P35Ppp17w6B6",
        "outputId": "946d9229-ce06-496f-ecbb-dd396ed50b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 단어 개수:  54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6va3w_nGw59w",
        "outputId": "0fe9384f-34eb-4fb3-cbd6-b0c41fe6a73c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'and': 1, 'from': 2, 'to': 3, 'in': 4, 'job': 5, 'the': 6, 'of': 7, 'we': 8, 'will': 9, 'now': 10, 'all': 11, 'yes': 12, 'she': 13, 'does': 14, 'mean': 15, \"everybody's\": 16, 'yours': 17, 'mine': 18, 'onward': 19, 'role': 20, 'grain': 21, 'farmers': 22, 'egypt': 23, 'pastry': 24, 'chefs': 25, 'paris': 26, 'dog': 27, 'walkers': 28, 'oregon': 29, 'i': 30, 'e': 31, 'every': 32, 'be': 33, 'able': 34, 'help': 35, 'direct': 36, 'workers’': 37, 'actions': 38, 'behavior': 39, 'with': 40, 'a': 41, 'new': 42, 'degree': 43, 'intelligence': 44, 'that': 45, 'comes': 46, 'predictive': 47, 'analytics': 48, 'stemming': 49, 'ai': 50, 'engines': 51, 'increasingly': 52, 'depend': 53, 'upon': 54}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자열 -> 인덱스 리스트\n",
        "tokenizer.texts_to_sequences([\"yes, she is in Paris\"]) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuhpekrow54Y",
        "outputId": "67e59e7c-86cb-4651-a059-ee58a2f53d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[12, 13, 4, 26]]"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sen = tokenizer.texts_to_sequences([[\"yes\", \"she\", \"is\", \"in\", \"Paris\"]])\n",
        "tokenized_sen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6dztqegw5yy",
        "outputId": "b28c019c-2697-4eee-a606-5fecf23f91f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[12, 13, 4, 26]]"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 최대 단어 지정하기 "
      ],
      "metadata": {
        "id": "NKtJiCU10ddO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 최대 단어 지정하기\n",
        "tokenizer = Tokenizer(num_words = 5)\n",
        "\n",
        "# 단어 인덱스 구축\n",
        "tokenizer.fit_on_texts([eng_text])"
      ],
      "metadata": {
        "id": "Z9KbInJRz1vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.num_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eqDnxU70gXP",
        "outputId": "ffd19382-a3e5-4ff5-97da-6b24f7a85a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.texts_to_sequences([[\"yes\", \"she\", \"is\", \"in\", \"Paris\"]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2nntCwr0bbZ",
        "outputId": "667d3c5b-98ad-472a-b5fa-053479c8dfea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[4]]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.8.2 Padding"
      ],
      "metadata": {
        "id": "My5VozZq03jT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "서로 다른 길이의 샘플들을 동일한 길이로 맞춰주기"
      ],
      "metadata": {
        "id": "c3wug12S09fB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 10 # 문장 최대 길이\n",
        "\n",
        "tokenized_sen = np.array(tokenized_sen)\n",
        "\n",
        "# 문장의 길이가 MAX_SEQUENCE_LENGTH 단어가 넘어가면 자르고, 모자르면 0으로 채워 넣는다.\n",
        "padded_sen = pad_sequences(tokenized_sen, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n",
        "\n",
        "print('Shape of input data tensor:', tokenized_sen.shape) # 리뷰 데이터의 형태 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtbSjTNO03fb",
        "outputId": "c44a1374-6585-4f37-eebf-83369b001c21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input data tensor: (1, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHU6ax_m03bS",
        "outputId": "4fd9111a-e325-43d4-8f90-e0961c9641e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[12, 13,  4, 26,  0,  0,  0,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkcMRwNYpBCu"
      },
      "source": [
        "#2. 한글 전처리 실습\n",
        "\n",
        "영문은 공백으로 토큰화가 가능하지만, 한글의 경우 품사를 고려하여 토큰화해야 한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq6znXciqK4u"
      },
      "source": [
        "## 2.1 실습용 한글기사 수집\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyLgGOc1qOXg"
      },
      "source": [
        "- https://n.news.naver.com/mnews/article/016/0002032876?sid=105"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL7tkeFmwtid"
      },
      "source": [
        "kor_text = '''포스코ICT는 25일 경기도 판교 사옥에서 청소년들이 개발한 인공지능(AI) 기반 애플리케이션을 직접 선보이는 ‘2022 AI 유스 챌린지(Youth Challenge)’를 개최했다. 청소년들은 기성세대들이 생각하지 못했던 참신한 아이디어를 바탕으로 AI 시스템을 개발해 관심을 모았다.\n",
        "\n",
        "예선을 거쳐 본선에 오른 전국 중·고교 7개팀은 지난 달부터 포스코ICT 소속 AI 엔지니어의 코칭 속에 개발한 AI 시스템을 직접 시연했다. 심사는 카이스트, 포스텍, 연세대 교수 등으로 구성된 평가위원들과 포스코ICT 직원들로 구성된 내부 평가위원들이 맡았다. 우수작은 과기정통부장관상(1팀), 포스코ICT사장상(1팀), 지능정보산업협회장상(1팀), 우수상(2팀), 장려상(2팀) 등으로 나눠 각각 시상했다. 포스코ICT는 AI 기술을 접목해 환경·안전·사회문제를 해결할 수 있는 청소년들의 아이디어를 발굴하기 위해 이번 공모전을 개최했다. 평소 AI 기술을 접할 기회가 부족했던 청소년들은 멘토링을 통해 산업현장에서 실제 활용되는 기술을 자연스럽게 체험하고 진로 결정에도 도움을 받을 수 있었다.\n",
        "\n",
        "이번 공모전에 참여한 김희주 학생은 “막연히 생각했던 아이디어를 실제 AI 전문가들과 구체화해나가며 접하지 못했던 지식과 기술을 배울 수 있었고 AI 분야에 더 큰 관심을 가지게 됐다”고 말했다.\n",
        "\n",
        "학생들의 멘토를 맡은 포스코ICT 최영철 연구원은 “학생들의 열정과 창의적인 아이디어를 접하며 새로운 자극을 받아 재미있게 멘토링에 참여했다”며 “그동안 AI 분야에서 쌓아온 재능을 기부해 AI 전문가를 꿈꾸는 청소년들에게 전해줄 수 있어 의미 있었다”고 밝혔다. 한편, 포스코ICT는 아주대학교 대학원과 ‘AI 전문인력 양성을 위한 업무협약’을 체결해 프로그램을 운영하고 있다. 지난 24일에는 인공지능학과 대학원 과정에 재학 중인 우수 인재를 선발해 장학금을 지급했다.\n",
        "\n",
        "이번에 선발된 학생에게는 졸업 때까지 매달 장학금을 지급하고, 포스코ICT의 AI 전문가와의 1대1 멘토링 및 각종 기술 교육, 세미나 참석을 비롯해 채용 기회까지 제공할 계획이다. 김현일 기자'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjdVP8sYw3Lt"
      },
      "source": [
        "## 2.2 한글 토큰화 및 형태소 분석"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi1n6knExDK8"
      },
      "source": [
        "한글 자연어 처리기 비교  \n",
        "https://konlpy.org/ko/latest/morph/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6V9Lc70xLxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e10e37a8-697c-4dcc-fae9-a72813242b30"
      },
      "source": [
        "#konlpy 설치\n",
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 5.6 MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 44.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgooNkeSxbTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a295d8a5-4a8a-4850-fd41-50eb6114862c"
      },
      "source": [
        "#코모란(Komoran) 토큰화\n",
        "\n",
        "from konlpy.tag import Komoran\n",
        "komoran = Komoran()\n",
        "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 꺠닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있다.\"\n",
        "komoran_tokens = komoran.morphs(kor_text)\n",
        "print(komoran_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하', '고', '있', '다는', '것', '을', '꺠닫지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능', '적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-hCuM8XxvNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af2d838b-b4c4-4e9c-90c9-1e6d53f56b2a"
      },
      "source": [
        "#한나눔(hannanum) 토큰화 \n",
        "from konlpy.tag import Hannanum\n",
        "hannanum = Hannanum()\n",
        "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 꺠닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있다.\"\n",
        "hannanum_tokens = hannanum.morphs(kor_text)\n",
        "print(hannanum_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하고', '있', '다는', '것', '을', '꺠닫지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqb3TjN-yBnp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21e16dd6-261f-4be2-9e18-c6b44d155405"
      },
      "source": [
        "#Okt 토큰화 \n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "okt_tokens = okt.morphs(kor_text)\n",
        "print(okt_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하고', '있다는', '것', '을', '꺠닫', '지', '못', '하고', '인간', '과', '대화', '를', '계속', '할', '수', '있다면', '컴퓨터', '는', '지능', '적', '인', '것', '으로', '간주', '될', '수', '있다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIeDF4EAyW2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e1310ee-bf79-4c65-e328-909bbd9596cc"
      },
      "source": [
        "# Kkma 토큰화\n",
        "from konlpy.tag import Kkma \n",
        "kkma = Kkma()\n",
        "kkma_tokens = kkma.morphs(kor_text)\n",
        "print(kkma_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하', '고', '있', '다는', '것', '을', '꺠', '닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능', '적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jigjdTzvyrCo"
      },
      "source": [
        "## 2.3 한글 품사 부착(PoS Tagging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLvx-C3Jyurq"
      },
      "source": [
        "Pos Tag 목록 \n",
        "https://docs.google.com/spreadsheets/u/1/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Uz-1t8FyzrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e752b362-619e-4621-9dfd-4348a4012033"
      },
      "source": [
        "# 코모란(Komoran) 품사 태깅\n",
        "komoranTag = []\n",
        "for token in komoran_tokens:\n",
        "  komoranTag += komoran.pos(token)\n",
        "print(komoranTag)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('인간', 'NNG'), ('이', 'MM'), ('컴퓨터', 'NNG'), ('오', 'VV'), ('아', 'EC'), ('대화', 'NNG'), ('하', 'NNG'), ('고', 'MM'), ('있', 'VV'), ('달', 'VV'), ('는', 'ETM'), ('것', 'NNB'), ('을', 'NNG'), ('꺠닫지', 'NA'), ('못', 'MAG'), ('하', 'MAG'), ('고', 'MM'), ('인간', 'NNG'), ('과', 'NNG'), ('대화', 'NNG'), ('를', 'JKO'), ('계속', 'MAG'), ('하', 'NNG'), ('ㄹ', 'NA'), ('수', 'NNB'), ('있', 'VV'), ('다면', 'NNG'), ('컴퓨터', 'NNG'), ('늘', 'VV'), ('ㄴ', 'ETM'), ('지능', 'NNP'), ('적', 'NNB'), ('이', 'MM'), ('ㄴ', 'JX'), ('것', 'NNB'), ('으로', 'JKB'), ('간주', 'NNG'), ('되', 'NNB'), ('ㄹ', 'NA'), ('수', 'NNB'), ('있', 'VV'), ('다', 'MAG'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5IRmwubzDSJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a5e7ed-c4d8-4228-87a8-586254d0daa4"
      },
      "source": [
        "# 한나눔(Hannanum) 품사 태깅\n",
        "hannanum_Tag = []\n",
        "for token in hannanum_tokens:\n",
        "  hannanum_Tag += hannanum.pos(token)\n",
        "\n",
        "print(hannanum_Tag)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('인간', 'N'), ('이', 'M'), ('컴퓨터', 'N'), ('와', 'I'), ('대화', 'N'), ('하', 'P'), ('고', 'E'), ('있', 'N'), ('다', 'M'), ('는', 'J'), ('것', 'N'), ('을', 'N'), ('꺠닫지', 'N'), ('못하', 'P'), ('어', 'E'), ('고', 'M'), ('인간', 'N'), ('과', 'N'), ('대화', 'N'), ('를', 'N'), ('계속', 'M'), ('하', 'I'), ('ㄹ', 'N'), ('수', 'N'), ('있', 'N'), ('다면', 'N'), ('컴퓨터', 'N'), ('늘', 'P'), ('ㄴ', 'E'), ('지능적', 'N'), ('이', 'M'), ('ㄴ', 'N'), ('것', 'N'), ('으', 'N'), ('로', 'J'), ('간주', 'N'), ('되', 'N'), ('ㄹ', 'N'), ('수', 'N'), ('있', 'N'), ('다', 'M'), ('.', 'S')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eKGCe0CzdDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a1903e-ea84-41f4-e10c-8d256a34dbee"
      },
      "source": [
        "# Okt 품사 태깅\n",
        "oktTag = []\n",
        "for token in okt_tokens:\n",
        "  oktTag += okt.pos(token)\n",
        "print(oktTag)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('인간', 'Noun'), ('이', 'Noun'), ('컴퓨터', 'Noun'), ('와', 'Verb'), ('대화', 'Noun'), ('하고', 'Verb'), ('있다는', 'Adjective'), ('것', 'Noun'), ('을', 'Josa'), ('꺠닫', 'Noun'), ('지', 'Verb'), ('못', 'Noun'), ('하고', 'Verb'), ('인간', 'Noun'), ('과', 'Noun'), ('대화', 'Noun'), ('를', 'Noun'), ('계속', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있다면', 'Adjective'), ('컴퓨터', 'Noun'), ('는', 'Verb'), ('지능', 'Noun'), ('적', 'Noun'), ('인', 'Noun'), ('것', 'Noun'), ('으로', 'Josa'), ('간주', 'Noun'), ('될', 'Verb'), ('수', 'Noun'), ('있다', 'Adjective'), ('.', 'Punctuation')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGjlKUtFzuxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf0456a9-d005-4867-ad57-870fab909c72"
      },
      "source": [
        "#Kkma 품사 태깅\n",
        "kkmaTag = []\n",
        "for token in kkma_tokens:\n",
        "  kkmaTag += kkma.pos(token)\n",
        "print(kkmaTag)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('인간', 'NNG'), ('이', 'NNG'), ('컴퓨터', 'NNG'), ('오', 'VA'), ('아', 'ECS'), ('대화', 'NNG'), ('하', 'NNG'), ('고', 'NNG'), ('있', 'VA'), ('달', 'VV'), ('는', 'ETD'), ('것', 'NNB'), ('을', 'NNG'), ('꺠', 'UN'), ('닫', 'VV'), ('지', 'NNG'), ('못하', 'VX'), ('고', 'NNG'), ('인간', 'NNG'), ('과', 'NNG'), ('대화', 'NNG'), ('를', 'UN'), ('계속', 'MAG'), ('하', 'NNG'), ('ㄹ', 'NNG'), ('수', 'NNG'), ('있', 'VA'), ('다면', 'NNG'), ('컴퓨터', 'NNG'), ('늘', 'VA'), ('ㄴ', 'ETD'), ('지능', 'NNG'), ('적', 'NNG'), ('이', 'NNG'), ('ㄴ', 'NNG'), ('것', 'NNB'), ('으', 'UN'), ('로', 'JKM'), ('간주', 'NNG'), ('되', 'VA'), ('ㄹ', 'NNG'), ('수', 'NNG'), ('있', 'VA'), ('다', 'NNG'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXRAxzi2z291"
      },
      "source": [
        "## 2.4 불용어 처리(StopWord) 처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj8IcYhbz8ht"
      },
      "source": [
        "분석에 불필요한 품사를 제거하고, 불필요한 단어(불용어)를 제거한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1k1_HYk0A2F"
      },
      "source": [
        "# 불용어 처리\n",
        "stopPos = ['Suffix','Punctuation','Josa','Foreign','Alpha','Number']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQiZkyyH0IEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54843628-d1a3-4d78-efcb-6e20dd565df3"
      },
      "source": [
        "#최빈어 조회: 최빈어를 조회하여 불용어 제거 대상을 선정\n",
        "from collections import Counter\n",
        "Counter(oktTag).most_common()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('인간', 'Noun'), 2),\n",
              " (('컴퓨터', 'Noun'), 2),\n",
              " (('대화', 'Noun'), 2),\n",
              " (('하고', 'Verb'), 2),\n",
              " (('것', 'Noun'), 2),\n",
              " (('수', 'Noun'), 2),\n",
              " (('이', 'Noun'), 1),\n",
              " (('와', 'Verb'), 1),\n",
              " (('있다는', 'Adjective'), 1),\n",
              " (('을', 'Josa'), 1),\n",
              " (('꺠닫', 'Noun'), 1),\n",
              " (('지', 'Verb'), 1),\n",
              " (('못', 'Noun'), 1),\n",
              " (('과', 'Noun'), 1),\n",
              " (('를', 'Noun'), 1),\n",
              " (('계속', 'Noun'), 1),\n",
              " (('할', 'Verb'), 1),\n",
              " (('있다면', 'Adjective'), 1),\n",
              " (('는', 'Verb'), 1),\n",
              " (('지능', 'Noun'), 1),\n",
              " (('적', 'Noun'), 1),\n",
              " (('인', 'Noun'), 1),\n",
              " (('으로', 'Josa'), 1),\n",
              " (('간주', 'Noun'), 1),\n",
              " (('될', 'Verb'), 1),\n",
              " (('있다', 'Adjective'), 1),\n",
              " (('.', 'Punctuation'), 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU7AEdyY0QfW"
      },
      "source": [
        "stopWord = ['의','이','로','두고','들','를','은','과','수','했다','것','있는','한다','하는','그','있다']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeSfa8A-0VEe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e72997b-bfc0-4e53-ce6b-6edd39b81857"
      },
      "source": [
        "word =[]\n",
        "for tag in oktTag:\n",
        "  if tag[1] not in stopPos:\n",
        "    if tag[0] not in stopWord:\n",
        "      word.append(tag[0])\n",
        "\n",
        "print(word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '컴퓨터', '와', '대화', '하고', '있다는', '꺠닫', '지', '못', '하고', '인간', '대화', '계속', '할', '있다면', '컴퓨터', '는', '지능', '적', '인', '간주', '될']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-8Pa14-8i4pF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}